import os
import re
import json
import streamlit as st
from agno.agent import Agent
from agno.models.perplexity import Perplexity
from agno.tools.arxiv import ArxivTools
from agno.tools.reasoning import ReasoningTools
from agno.run.response import RunResponse, RunEvent

# ==== æŒä¹…åŒ–ç›¸å…³å‡½æ•° ====
REPORT_FILE = "weekly_report.json"

def save_weekly_report():
    data = {
        "response_content": st.session_state.get("response_content", ""),
        "citations_html": st.session_state.get("citations_html", ""),
        "paper_titles": st.session_state.get("paper_titles", []),
        "paper_abstracts": st.session_state.get("paper_abstracts", []),
        "paper_urls": st.session_state.get("paper_urls", []),
    }
    with open(REPORT_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_weekly_report():
    if os.path.exists(REPORT_FILE):
        with open(REPORT_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            st.session_state.response_content = data.get("response_content", "")
            st.session_state.citations_html = data.get("citations_html", "")
            st.session_state.paper_titles = data.get("paper_titles", [])
            st.session_state.paper_abstracts = data.get("paper_abstracts", [])
            st.session_state.paper_urls = data.get("paper_urls", [])

# ==== å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æŒä¹…åŒ–å†…å®¹ ====
load_weekly_report()

# è®¾ç½®APIå¯†é’¥
api_key = os.environ.get("OPENAI_API_KEY", "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")
os.environ["OPENAI_API_KEY"] = api_key

st.title("ArXiv å­¦æœ¯å‘¨æŠ¥ç”Ÿæˆå™¨")
st.write("æ ¹æ®å…³é”®è¯è‡ªåŠ¨ç”Ÿæˆæœ€æ–°arXivè®ºæ–‡å‘¨æŠ¥")

# åˆå§‹åŒ–å…³é”®è¯çŠ¶æ€
if "keywords" not in st.session_state:
    st.session_state.keywords = ["é€šä¿¡", "AI", "6G", "LLM", "Agent"]

# åˆå§‹åŒ–å†…å®¹çŠ¶æ€
if "response_content" not in st.session_state:
    st.session_state.response_content = ""
if "citations_html" not in st.session_state:
    st.session_state.citations_html = ""
if "paper_titles" not in st.session_state:
    st.session_state.paper_titles = []
if "paper_abstracts" not in st.session_state:
    st.session_state.paper_abstracts = []
if "paper_urls" not in st.session_state:
    st.session_state.paper_urls = []

st.subheader("ç®¡ç†å…³é”®è¯")
try:
    from streamlit_tags import st_tags
    st.session_state.keywords = st_tags(
        label='è¾“å…¥å…³é”®è¯:',
        text='è¾“å…¥åæŒ‰å›è½¦æ·»åŠ ',
        value=st.session_state.keywords,
        suggestions=['é‡å­è®¡ç®—', 'ç¥ç»ç½‘ç»œ', 'å¼ºåŒ–å­¦ä¹ '],
        maxtags=10,
        key="keywords_input"
    )
except ImportError:
    keyword_cols = st.columns(5)
    for i, kw in enumerate(st.session_state.keywords):
        with keyword_cols[i % 5]:
            if st.button(f"âŒ {kw}", key=f"del_{i}"):
                st.session_state.keywords.remove(kw)
                st.experimental_rerun()
    col1, col2 = st.columns([3, 1])
    with col1:
        new_keyword = st.text_input("æ·»åŠ æ–°å…³é”®è¯")
    with col2:
        if st.button("æ·»åŠ ") and new_keyword:
            st.session_state.keywords.append(new_keyword)
            st.experimental_rerun()

tab1, tab3, tab2 = st.tabs(["ğŸ“„ å­¦æœ¯å‘¨æŠ¥", "ğŸ“‘ è®ºæ–‡è¯¦æƒ…", "ğŸ”— å‚è€ƒæ–‡çŒ®"])

# ==== è‡ªåŠ¨ç”Ÿæˆå‘¨æŠ¥ï¼ˆä»…é¦–æ¬¡ï¼Œæ— å†…å®¹æ—¶ï¼‰ ====
if not st.session_state.response_content and st.session_state.keywords:
    with st.spinner("æ­£åœ¨è‡ªåŠ¨ç”Ÿæˆå­¦æœ¯å‘¨æŠ¥..."):
        agent = Agent(
            model=Perplexity(id="sonar-pro"),
            tools=[
                ArxivTools(search_arxiv=True, read_arxiv_papers=True),
                ReasoningTools(add_instructions=True)
            ],
            instructions=[
                "æ ¹æ®æä¾›çš„å…³é”®è¯ï¼Œæ£€ç´¢è¿‘ä¸€å‘¨arXivä¸Šç›¸å…³è®ºæ–‡ï¼Œç­›é€‰é«˜ç›¸å…³æ€§å†…å®¹ã€‚",
                "ç”¨è¡¨æ ¼åˆ—å‡ºè®ºæ–‡æ ‡é¢˜ã€ä½œè€…ã€å‘è¡¨æ—¶é—´ã€æ‘˜è¦è¦ç‚¹å’Œé“¾æ¥ï¼ˆç”¨è¡¨æ ¼å±•ç¤ºï¼‰",
                "æ­£æ–‡ä¸­å¼•ç”¨è®ºæ–‡ä»…ç”¨ç¼–å·æ ‡è®°ï¼Œåœ¨[ç¼–å·]æ ‡è®°ä¸­æ’å…¥è¶…é“¾æ¥ã€‚",
                "æœ€åç»™å‡ºè¶‹åŠ¿åˆ†æå’Œç®€è¦ç‚¹è¯„ã€‚"
            ],
            markdown=True,
            show_tool_calls=True
        )
        keywords_str = ", ".join(st.session_state.keywords)
        prompt = f"""
        è¯·æ ¹æ®å¦‚ä¸‹å…³é”®è¯ï¼Œç”Ÿæˆä¸€ä»½è¿‘ä¸€å‘¨arXivè®ºæ–‡å­¦æœ¯å‘¨æŠ¥ï¼Œå†…å®¹åŒ…æ‹¬ï¼š
        1. è®ºæ–‡æ ‡é¢˜ã€ä½œè€…ã€å‘è¡¨æ—¶é—´ã€æ‘˜è¦è¦ç‚¹å’Œé“¾æ¥(ç”¨è¡¨æ ¼å±•ç¤º)
        2. ç ”ç©¶è¶‹åŠ¿ç®€è¦åˆ†æ
        3. å­¦æœ¯çƒ­ç‚¹ç‚¹è¯„
        å…³é”®è¯ï¼š{keywords_str}
        """

        # æ¸…ç©ºæ—§å†…å®¹
        st.session_state.response_content = ""
        st.session_state.citations_html = ""
        st.session_state.paper_titles = []
        st.session_state.paper_abstracts = []
        st.session_state.paper_urls = []

        response_content = ""
        citations_html = ""

        # è¿™é‡Œä¸å†ç”¨ placeholderï¼Œç›´æ¥å†™å…¥ session_state
        for resp in agent.run(message=prompt, stream=True):
            if isinstance(resp, RunResponse) and resp.event == RunEvent.run_response and isinstance(resp.content, str):
                response_content += resp.content
            if resp.citations and resp.citations.urls:
                citations_html = "<ol>"
                for citation in resp.citations.urls:
                    if citation.url:
                        citations_html += f'<li><a href="{citation.url}" target="_blank">{citation.title or citation.url}</a></li>'
                citations_html += "</ol>"

        # å†™å…¥ session_state
        st.session_state.response_content = response_content
        st.session_state.citations_html = citations_html

        # æå–è®ºæ–‡æ ‡é¢˜ã€æ‘˜è¦ã€é“¾æ¥
        table_pattern = r"\|(.+?)\|\s*\n\|(?:[-:\s|]+)\|\s*\n((?:\|.*\|\s*\n?)+)"
        match = re.search(table_pattern, response_content, re.DOTALL)
        paper_titles = []
        paper_abstracts = []
        paper_urls = []
        if match:
            table_body = match.group(2)
            for line in table_body.strip().split("\n"):
                cols = [col.strip() for col in line.strip().strip("|").split("|")]
                if cols:
                    paper_titles.append(cols[0])
                    if len(cols) > 3:
                        paper_abstracts.append(cols[3])
                    else:
                        paper_abstracts.append("")
                    if len(cols) > 4:
                        paper_urls.append(cols[4])
                    else:
                        paper_urls.append("")
        st.session_state.paper_titles = paper_titles
        st.session_state.paper_abstracts = paper_abstracts
        st.session_state.paper_urls = paper_urls

        # === è‡ªåŠ¨ä¿å­˜ ===
        save_weekly_report()

# ==== ç”¨æˆ·ä¸»åŠ¨ç‚¹å‡»ç”ŸæˆæŒ‰é’® ====
if st.button("ç”Ÿæˆå­¦æœ¯å‘¨æŠ¥", type="primary"):
    if not st.session_state.keywords:
        st.error("è¯·è‡³å°‘æ·»åŠ ä¸€ä¸ªå…³é”®è¯")
    else:
        progress = st.progress(0)
        status = st.empty()
        status.info("å¼€å§‹å¤„ç†...")

        agent = Agent(
            model=Perplexity(id="sonar-pro"),
            tools=[
                ArxivTools(search_arxiv=True, read_arxiv_papers=True),
                ReasoningTools(add_instructions=True)
            ],
            instructions=[
                "æ ¹æ®æä¾›çš„å…³é”®è¯ï¼Œæ£€ç´¢è¿‘ä¸€å‘¨arXivä¸Šç›¸å…³è®ºæ–‡ï¼Œç­›é€‰é«˜ç›¸å…³æ€§å†…å®¹ã€‚",
                "ç”¨è¡¨æ ¼åˆ—å‡ºè®ºæ–‡æ ‡é¢˜ã€ä½œè€…ã€å‘è¡¨æ—¶é—´ã€æ‘˜è¦è¦ç‚¹å’Œé“¾æ¥ï¼ˆç”¨è¡¨æ ¼å±•ç¤ºï¼‰",
                "æ­£æ–‡ä¸­å¼•ç”¨è®ºæ–‡ä»…ç”¨ç¼–å·æ ‡è®°ï¼Œåœ¨[ç¼–å·]æ ‡è®°ä¸­æ’å…¥è¶…é“¾æ¥ã€‚",
                "æœ€åç»™å‡ºè¶‹åŠ¿åˆ†æå’Œç®€è¦ç‚¹è¯„ã€‚"
            ],
            markdown=True,
            show_tool_calls=True
        )
        keywords_str = ", ".join(st.session_state.keywords)
        prompt = f"""
        è¯·æ ¹æ®å¦‚ä¸‹å…³é”®è¯ï¼Œç”Ÿæˆä¸€ä»½è¿‘ä¸€å‘¨arXivè®ºæ–‡å­¦æœ¯å‘¨æŠ¥ï¼Œå†…å®¹åŒ…æ‹¬ï¼š
        1. è®ºæ–‡æ ‡é¢˜ã€ä½œè€…ã€å‘è¡¨æ—¶é—´ã€æ‘˜è¦è¦ç‚¹å’Œé“¾æ¥(ç”¨è¡¨æ ¼å±•ç¤º)
        2. ç ”ç©¶è¶‹åŠ¿ç®€è¦åˆ†æ
        3. å­¦æœ¯çƒ­ç‚¹ç‚¹è¯„
        å…³é”®è¯ï¼š{keywords_str}
        """

        # æ¸…ç©ºæ—§å†…å®¹
        st.session_state.response_content = ""
        st.session_state.citations_html = ""
        st.session_state.paper_titles = []
        st.session_state.paper_abstracts = []
        st.session_state.paper_urls = []

        response_content = ""
        citations_html = ""

        try:
            status.text("æ­£åœ¨æœç´¢ArXivè®ºæ–‡...")
            progress.progress(10)

            for i, resp in enumerate(agent.run(message=prompt, stream=True)):
                if isinstance(resp, RunResponse):
                    progress_value = min(85, 10 + (i * 2))
                    progress.progress(progress_value)

                    if progress_value < 30:
                        status.text("æ­£åœ¨æœç´¢ArXivè®ºæ–‡...")
                    elif progress_value < 60:
                        status.text("æ­£åœ¨åˆ†æè®ºæ–‡å†…å®¹...")
                    else:
                        status.text("æ­£åœ¨ç”Ÿæˆå‘¨æŠ¥...")
                    if resp.event == RunEvent.run_response and isinstance(resp.content, str):
                        response_content += resp.content

                    if resp.citations and resp.citations.urls:
                        citations_html = "<ol>"
                        for citation in resp.citations.urls:
                            if citation.url:
                                citations_html += f'<li><a href="{citation.url}" target="_blank">{citation.title or citation.url}</a></li>'
                        citations_html += "</ol>"

            # ç”Ÿæˆå®Œæˆï¼Œè¿›åº¦100%
            progress.progress(100)
            status.success("âœ… å­¦æœ¯å‘¨æŠ¥ç”Ÿæˆå®Œæˆ!")

            # å†™å…¥ session_state
            st.session_state.response_content = response_content
            st.session_state.citations_html = citations_html if citations_html else ""

            # æå–è®ºæ–‡æ ‡é¢˜ã€æ‘˜è¦ã€é“¾æ¥
            table_pattern = r"\|(.+?)\|\s*\n\|(?:[-:\s|]+)\|\s*\n((?:\|.*\|\s*\n?)+)"
            match = re.search(table_pattern, response_content, re.DOTALL)
            paper_titles = []
            paper_abstracts = []
            paper_urls = []
            if match:
                table_body = match.group(2)
                for line in table_body.strip().split("\n"):
                    cols = [col.strip() for col in line.strip().strip("|").split("|")]
                    if cols:
                        paper_titles.append(cols[0])
                        if len(cols) > 3:
                            paper_abstracts.append(cols[3])
                        else:
                            paper_abstracts.append("")
                        if len(cols) > 4:
                            paper_urls.append(cols[4])
                        else:
                            paper_urls.append("")
            st.session_state.paper_titles = paper_titles
            st.session_state.paper_abstracts = paper_abstracts
            st.session_state.paper_urls = paper_urls

            # === æŒä¹…åŒ–ä¿å­˜ ===
            save_weekly_report()

        except Exception as e:
            progress.progress(100)
            status.error(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

# =================== TAB æ¸²æŸ“ ===================

with tab1:
    # æ¯æ¬¡æ¸²æŸ“éƒ½æ ¹æ® session_state å±•ç¤ºå†…å®¹
    if st.session_state.response_content:
        st.markdown(st.session_state.response_content)
    else:
        st.markdown("è¯·å…ˆç”Ÿæˆå­¦æœ¯å‘¨æŠ¥ã€‚")

with tab2:
    st.markdown(st.session_state.citations_html or "æš‚æ— å‚è€ƒæ–‡çŒ®ã€‚", unsafe_allow_html=True)

with tab3:
    if st.session_state.paper_titles:
        for idx, title in enumerate(st.session_state.paper_titles):
            abstract = st.session_state.paper_abstracts[idx] if idx < len(st.session_state.paper_abstracts) else ""
            url = st.session_state.paper_urls[idx] if idx < len(st.session_state.paper_urls) else ""
            expanded_key = f"expander_{idx}"
            if expanded_key not in st.session_state:
                st.session_state[expanded_key] = False

            chat_key = f"chat_history_{idx}"
            if chat_key not in st.session_state:
                default_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç§‘ç ”åŠ©ç†ï¼Œæ“…é•¿æ·±å…¥è§£è¯»å­¦æœ¯è®ºæ–‡ã€‚è¯·æ ¹æ®ä»¥ä¸‹è®ºæ–‡é“¾æ¥ï¼Œè”ç½‘æ£€ç´¢å¹¶é˜…è¯»è®ºæ–‡åŸæ–‡ï¼Œæä¾›è¯¦ç»†çš„æŠ€æœ¯è§£è¯»ã€‚è§£è¯»å†…å®¹åº”åŒ…æ‹¬ï¼š

- ç ”ç©¶èƒŒæ™¯ä¸é—®é¢˜
- ç ”ç©¶æ–¹æ³•ä¸æŠ€æœ¯è·¯çº¿
- ä¸»è¦åˆ›æ–°ç‚¹
- å®éªŒè®¾è®¡ä¸ç»“æœ
- ç ”ç©¶ç»“è®ºåŠå…¶å­¦æœ¯ä¸å®é™…æ„ä¹‰

è¯·ç”¨ä¸­æ–‡æ’°å†™ï¼Œé¢å‘é¢†åŸŸå†…ä¸“å®¶ï¼Œå†…å®¹å‡†ç¡®ä¸”æ·±å…¥ã€‚

è®ºæ–‡æ ‡é¢˜ï¼š{title}
è®ºæ–‡é“¾æ¥ï¼š{url}

è¯·ç»™å‡ºè¯¦ç»†è§£è¯»ï¼š
"""
                st.session_state[chat_key] = [
                    {"role": "assistant", "content": default_prompt}
                ]

            with st.expander(f"{idx+1}. {title}", expanded=st.session_state[expanded_key]):
                if url:
                    st.markdown(f"**æ‘˜è¦ï¼š** {abstract}\n\n**é“¾æ¥ï¼š** [{url}]")
                else:
                    st.markdown(f"**æ‘˜è¦ï¼š** {abstract}\n\n**é“¾æ¥ï¼š** æš‚æ— é“¾æ¥")

                for msg in st.session_state[chat_key]:
                    with st.chat_message(msg["role"]):
                        st.markdown(msg["content"])

                user_input = st.chat_input("è¯·è¾“å…¥ä½ æƒ³é’ˆå¯¹æœ¬è®ºæ–‡æé—®çš„é—®é¢˜â€¦", key=f"chat_input_{idx}")
                if user_input:
                    st.session_state[chat_key].append({"role": "user", "content": user_input})

                    agent = Agent(
                        model=Perplexity(id="sonar-pro"),
                        tools=[
                            ArxivTools(search_arxiv=False, read_arxiv_papers=False),
                            ReasoningTools(add_instructions=True)
                        ],
                        instructions=[],
                        markdown=True,
                        show_tool_calls=False
                    )
                    prompt = "\n".join(
                        [f"{m['role']}: {m['content']}" for m in st.session_state[chat_key]]
                    )
                    response_content = ""
                    chat_placeholder = st.empty()
                    with st.chat_message("assistant"):
                        for resp in agent.run(message=prompt, stream=True):
                            if isinstance(resp, RunResponse) and resp.event == RunEvent.run_response and isinstance(resp.content, str):
                                response_content += resp.content
                                chat_placeholder.markdown(response_content)
                    st.session_state[chat_key].append({"role": "assistant", "content": response_content})
    else:
        st.write("æœªæ£€æµ‹åˆ°è®ºæ–‡æ ‡é¢˜è¡¨æ ¼ï¼Œæ— æ³•å±•ç¤ºè®ºæ–‡è¯¦æƒ…ã€‚")
